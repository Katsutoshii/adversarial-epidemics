{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/sucharitajayanti/Documents/W'20/CS 189 - Network Science and Complex Systems/finalProject/adversarial-epidemics/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import datapackage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from simulator import SIRD, SIRDGraph\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yakoob's Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Python Helper functions '''\n",
    "\n",
    "def load_data(filepath):\n",
    "    # load data into pandas dataframe\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "# creates the global airport network (domestic and international)\n",
    "def flight_graph(df):\n",
    "    # Create directed graph from routes data\n",
    "    G = nx.DiGraph()\n",
    "    for index, row in df.iterrows(): \n",
    "        G.add_edge(row['Source ID'], row['Destination ID']) \n",
    "        \n",
    "    return G\n",
    "\n",
    "\n",
    "def summarize_graph(G):\n",
    "    # Summary statistics about the network\n",
    "    return nx.info(G)\n",
    "\n",
    "\n",
    "def plot_degree_distribution(G):\n",
    "    # extract degree of each airport sorted in decreasing order\n",
    "    degrees = [airport[1] for airport in sorted(nx.degree(G), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    plt.plot(degrees)\n",
    "\n",
    "\n",
    "# retrieves the country name given an ID\n",
    "def country_dict(df2):\n",
    "    # Get the country name from ID number\n",
    "    country = dict()\n",
    "\n",
    "    for index, row in df2.iterrows(): \n",
    "        country[row['Airport ID']] = row['Country']\n",
    "\n",
    "    return country\n",
    "\n",
    "\n",
    "# helper function to retrieve country name given an airport ID\n",
    "def get_country(ID, country):\n",
    "    # @return country_name - str\n",
    "    if ID == \"\\\\N\" or float(ID) not in country:\n",
    "        return 'No ID'\n",
    "\n",
    "    return country[float(ID)]\n",
    "\n",
    "# helper function used to rank countries by number of airports\n",
    "def num_airports(G, country):\n",
    "    num_airports = dict()\n",
    "\n",
    "    for airport in  nx.degree(G):\n",
    "        name = get_country(airport[0], country)\n",
    "        if name != 'No ID':\n",
    "            num_airports[name] = num_airports.get(name, 0) + 1\n",
    "\n",
    "    names, num = [], [] \n",
    "    for name, count in sorted(num_airports.items(), key=lambda item: item[1], reverse=True):\n",
    "        names.append(name)\n",
    "        num.append(count)\n",
    "        \n",
    "#     d = {'Country': names, 'Number of Airports': num}\n",
    "#     country_df = pd.DataFrame(data=d)\n",
    "#     country_df.head(10)\n",
    "    \n",
    "    return names, num\n",
    "\n",
    "# Main function used to generate the country graph\n",
    "def create_country_graph(df, country):\n",
    "    # International graph network, weighted by number of routes between two countries\n",
    "    country_graph = nx.DiGraph()\n",
    "    max_weight = 0\n",
    "    \n",
    "    for index, row in df.iterrows(): \n",
    "        source, destination = get_country(row['Source ID'],country), get_country(row['Destination ID'],country)\n",
    "        if source != 'No ID' and destination != 'No ID':\n",
    "            if country_graph.has_edge(source, destination):\n",
    "                country_graph[source][destination]['weight'] += 1\n",
    "            else:\n",
    "                country_graph.add_edge(source, destination, weight=1) \n",
    "\n",
    "            max_weight = max(max_weight, country_graph[source][destination]['weight'])\n",
    "    \n",
    "    return country_graph, max_weight\n",
    "\n",
    "def load_datasets():\n",
    "    df = load_data('../data/routes.txt')\n",
    "    df = df.drop(['Airline', 'Airline ID', 'Codeshare', 'Stops', 'Equipment'], axis=1)\n",
    "\n",
    "    # Read airport data to map Airport ID -> Country\n",
    "    df2 = load_data('../data/airports.txt')\n",
    "    df2 = df2.drop(['Latitude','Longitude','Altitude','Timezone','DST',\n",
    "                    'database time zone','Type','Source','City','Name'], axis=1) \n",
    "    \n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the routes and airport csv files into pandas dataframes\n",
    "df, df2 = load_datasets()\n",
    "\n",
    "# create a id to country mapping\n",
    "country = country_dict(df2)\n",
    "\n",
    "# create country graph\n",
    "G, max_weight = create_country_graph(df, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sucharita's Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(G, D, string, default_val):\n",
    "    missed = np.setdiff1d(list(G.nodes()), list(D.keys()))\n",
    "    if(len(missed)):\n",
    "        print(\"missing \" + string + \" for the following locations:\")\n",
    "        print(missed)\n",
    "        for i in missed:\n",
    "            D[i] = default_val\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates what the weight should be on edge (n1,n2) n1!=n2\n",
    "# Will get more complicated eventually\n",
    "\n",
    "def get_edge_weight(base_weight, w_n1, w_n2):\n",
    "    return base_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_with_labels(G, pop_dict, node_wreg, spread_rate, mortality_rate, recovery, infected, pos, max_weight):\n",
    "    \n",
    "    pop_dict = clean_up_data(G, pop_dict, \"populations\", 0)\n",
    "    node_wreg = clean_up_data(G, node_wreg, \"weight regulations\", 1)\n",
    "    spread_rate = clean_up_data(G, spread_rate, \"b (spread rate)\", 0)\n",
    "    mortality_rate = clean_up_data(G, mortality_rate, \"w (mortality rate)\", 0)\n",
    "    recovery = clean_up_data(G, recovery, \"k (recovery)\", 0.5)\n",
    "    infected = clean_up_data(G, infected, \"i (% initially infected)\", 0)\n",
    "    pos = clean_up_data(G, pos, \"positions\", (0,0))\n",
    "    \n",
    "    max_weight = max_weight + 0.0000000001\n",
    "\n",
    "    nodes = [(n, SIRD(b=spread_rate[n], k=recovery[n], w=mortality_rate[n], N=pop_dict[n], i=infected[n]), pos[n]) for n in list(G.nodes())] \n",
    "    edges = [(u,v,get_edge_weight(d['weight'], node_wreg[u], node_wreg[v])/max_weight) for (u,v,d) in G.edges(data=True)]\n",
    "    \n",
    "    return nodes, edges\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population Related Functions\n",
    "\n",
    "def get_pop_data():\n",
    "    data_url = 'https://datahub.io/JohnSnowLabs/population-figures-by-country/datapackage.json'\n",
    "\n",
    "    # to load Data Package into storage\n",
    "    package = datapackage.Package(data_url)\n",
    "\n",
    "    # to load only tabular data\n",
    "    resources = package.resources\n",
    "    for resource in resources:\n",
    "        if resource.tabular:\n",
    "            data = pd.read_csv(resource.descriptor['path'])\n",
    "    return data\n",
    "\n",
    "def pop_dict_year(data, year):\n",
    "    label = 'Year_' + str(year)\n",
    "    df = data[['Country',label]]\n",
    "    pop_dict = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        pop_dict[df.iloc[i][0]] = df.iloc[i][1]\n",
    "\n",
    "    return pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "missing populations for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\nmissing weight regulations for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\nmissing b (spread rate) for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\nmissing w (mortality rate) for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\nmissing k (recovery) for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\nmissing i (% initially infected) for the following locations:\n['Bahamas' 'British United States' 'Brunei' 'Cape Verde'\n 'Congo (Brazzaville)' 'Congo (Kinshasa)' 'Cook Islands' 'Egypt' 'Gambia'\n 'Iran' 'Kyrgyzstan' 'Laos' 'Macau' 'Macedonia' 'Micronesia' 'Niue'\n 'North Korea' 'Russia' 'Saint Kitts and Nevis' 'Saint Lucia'\n 'Saint Vincent and the Grenadines' 'Slovakia' 'South Korea' 'Taiwan'\n 'Venezuela' 'Western Sahara' 'Yemen']\n"
    }
   ],
   "source": [
    "pop_dict = pop_dict_year(get_pop_data(), 2003)\n",
    "pop_dict[\"Hong Kong\"] = pop_dict['Hong Kong SAR, China']\n",
    "\n",
    "node_wreg = {k:1 for k in pop_dict}\n",
    "spread_rate = {k:0.5 for k in pop_dict}\n",
    "mortality_rate = {k:0.1 for k in pop_dict}\n",
    "recovery = {k:0.8 for k in pop_dict}\n",
    "infected = {k:0 for k in pop_dict}\n",
    "infected[\"Germany\"] = 1\n",
    "infected[\"Canada\"] = 8\n",
    "infected[\"Singapore\"] = 20 \n",
    "infected[\"Hong Kong\"] = 95\n",
    "infected[\"Switzerland\"] = 2\n",
    "infected[\"Thailand\"] = 1\n",
    "infected[\"Vietnam\"] =  40\n",
    "infected[\"China\"] = 100\n",
    "\n",
    "percent_infected = {n:infected[n]/pop_dict[n] for n in infected}\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "\n",
    "n, e = get_graph_with_labels(G, pop_dict, node_wreg, spread_rate, mortality_rate, recovery, percent_infected, pos, max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('Singapore', SIRD(b=0.5, k=0.8, w=0.1, N=4114826.0, s=0.9, i=4.860472836518482e-06, r=0, d=0, xi=0, records=Records(plabels=['susceptible', 'cases', 'recoveries', 'deaths'], pcolors=['blue', 'red', 'yellow', 'black'], records=[[], [], [], []])), array([-0.00788965, -0.15829407]))\n"
    }
   ],
   "source": [
    "# For checking values\n",
    "for tup in n:\n",
    "    if tup[0] == \"Singapore\":\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import SIRDGraph\n",
    "srirdg: SIRDGraph = SIRDGraph(n, e,\n",
    "    width_factor=1/2,\n",
    "    size_factor=1/400000,\n",
    "    font_size=1,\n",
    "    color_factor=100000,\n",
    "    y_offset=0.01,\n",
    "    font_color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "srirdg.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srirdg.save_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}